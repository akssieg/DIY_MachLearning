{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as d_sets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes ###\n",
    "---\n",
    "* **Discrete** feature vectors \n",
    "* The model makes a strong assumption that $x_{i}$ and y are *conditionally independet* \n",
    "\n",
    "> ###### Naive Bayes assumption\n",
    "> For instance, if y = 1 means spam email; “buy” is word 2087 and “price” is word 39831; \n",
    "> then we are assuming that if I tell you y = 1 (that a particular piece of email is spam), then knowledge\n",
    "> of $x_{2087}$ (knowledge of whether “buy” appears in the message) will have no effect on your beliefs about \n",
    "> the value of $x_{39831}$ (whether “price” appears).\n",
    "\n",
    "\n",
    "> **More formally, this can be written:**  \n",
    "> p($x_{2087}$ | y) = p($x_{2087}$|y, $x_{39831}$).  \n",
    "> (Note that this is not the same as saying that $x_{2087}$ and $x_{39831}$ are independent,  \n",
    "> which would have been written \"p($x_{2087}$) = p($x_{2087}$|$x_{39831}$\";  \n",
    "> rather, we are only assuming that $x_{2087}$ $x_{39831}$ are conditionally independent given y.)\n",
    "\n",
    "Good example - spam classifier where feature vector = bianry dictionary vector  \n",
    "taken from [ AndrewNG-s cs299 notes ](http://cs229.stanford.edu/notes/cs229-notes2.pdf)  \n",
    "also from [machinelearningmastery](http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadData(filename, splitRatio):\n",
    "    dataset = np.loadtxt(filename, delimiter=',')\n",
    "    \n",
    "    train_size = int(dataset.shape[0] * splitRatio)\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    return (dataset[0:train_size,:], dataset[train_size:,:])\n",
    "\n",
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "        \n",
    "    def separateByClass(self,dataset):\n",
    "        separated = {}\n",
    "        for i in range(dataset.shape[0]):\n",
    "            vector = dataset[i]\n",
    "            if (vector[-1] not in separated):\n",
    "                separated[vector[-1]] = []\n",
    "            separated[vector[-1]].append(vector)\n",
    "        return separated\n",
    "\n",
    "    def stdev(self,numbers):\n",
    "        avg = np.mean(numbers)\n",
    "        variance = np.mean([np.power(x-avg,2) for x in numbers])\n",
    "        return np.sqrt(variance)\n",
    "\n",
    "    def summarize(self,dataset):\n",
    "        return [(np.mean(attribute), self.stdev(attribute)) for attribute in zip(*dataset)][:-1]\n",
    "\n",
    "    def fit(self,dataset):\n",
    "        separated = separateByClass(dataset)\n",
    "        for classValue, instances in separated.iteritems():\n",
    "            self.summaries[classValue] = summarize(instances)\n",
    "\n",
    "    def getProb(self,x, mean, stdev):\n",
    "        exponent = np.exp(-(np.power(x-mean,2)/(2*np.power(stdev,2))))\n",
    "        return (1 / (np.sqrt(2*np.pi) * stdev)) * exponent\n",
    "\n",
    "    def calculateClassProbabilities(self, inputVector):\n",
    "        probabilities = {}\n",
    "        for classValue, classSummaries in self.summaries.iteritems():\n",
    "            probabilities[classValue] = 1\n",
    "            for i in range(len(classSummaries)):\n",
    "                mean, stdev = self.classSummaries[i]\n",
    "                x = inputVector[i]\n",
    "                probabilities[classValue] *= self.getProb(x, mean, stdev)\n",
    "        return probabilities\n",
    "\n",
    "    def _predict(self, inputVector):\n",
    "        probabilities = calculateClassProbabilities(self.summaries, inputVector)\n",
    "        bestLabel, bestProb = None, -1\n",
    "        for classValue, probability in probabilities.iteritems():\n",
    "            if bestLabel is None or probability > bestProb:\n",
    "                bestProb = probability\n",
    "                bestLabel = classValue\n",
    "        return bestLabel\n",
    "\n",
    "    def predict(self, testSet):\n",
    "        return [predict(self.summaries, tS) for tS in testSet]\n",
    "\n",
    "def accuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 768 rows into train=460 and test=308 rows\n",
      "Accuracy: 73.3766233766%\n"
     ]
    }
   ],
   "source": [
    "filename = '../datasets/pima-indians-diabetes.csv'\n",
    "splitRatio = 0.6\n",
    "\n",
    "trainingSet, testSet = loadData(filename, splitRatio)\n",
    "print('Split {0} rows into train={1} and test={2} rows').format(len(trainingSet)+len(testSet), len(trainingSet), len(testSet))\n",
    "\n",
    "clf = NaiveBayes()\n",
    "summaries = clf.fit(trainingSet)\n",
    "\n",
    "predictions = clf.predict(testSet)\n",
    "\n",
    "acc = accuracy(testSet, predictions)\n",
    "print('Accuracy: {0}%').format(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
